{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gibbs Introduction Welcome to the documentation of the gibbs package. gibbs is a package that help you scale your ML workers (or any python code) across processes and machines, asynchronously. gibbs is : \u26a1\ufe0f Highly performant \ud83d\udd00 Asynchronous \ud83d\udc25 Easy-to-use Installation Latest version You can install the latest stable version of the package directly from PyPi with : pip install gibbs Bleeding-edge version To install the bleeding-edge version ( main , not released), you can do : pip install git+https://github.com/astariul/gibbs.git Local version For development purposes, you can clone the repository locally and install it manually : git clone https://github.com/astariul/gibbs.git cd gibbs pip install -e . Extra dependencies You can also install extras dependencies, for example : pip install gibbs [ docs ] Will install necessary dependencies for building the docs. List of extra dependencies : test : Dependencies for running unit-tests. hook : Dependencies for running pre-commit hooks. lint : Dependencies for running linters and formatters. docs : Dependencies for building the documentation. ex : Dependencies for running the examples. dev : test + hook + lint + docs . all : All extra dependencies. Contribute To contribute, install the package locally (see Installation ), create your own branch, add your code (and tests, and documentation), and open a PR ! Pre-commit hooks Pre-commit hooks are set to check the code added whenever you commit something. When you try to commit your code, hooks are automatically run, and if you code does not meet the quality required by linters, it will not be committed. You then have to fix your code and try to commit again ! Important If you never ran the hooks before, install it with : pre-commit install Info You can manually run the pre-commit hooks with : pre-commit run --all-files Unit-tests When you contribute, you need to make sure all the unit-tests pass. You should also add tests if necessary ! You can run the tests with : pytest Info Pre-commit hooks will not run the tests, but it will automatically update the coverage badge ! Documentation When you contribute, make sure to keep the documentation up-to-date. You can visualize the documentation locally by running : mkdocs serve","title":"Welcome"},{"location":"#gibbs","text":"","title":"Gibbs"},{"location":"#introduction","text":"Welcome to the documentation of the gibbs package. gibbs is a package that help you scale your ML workers (or any python code) across processes and machines, asynchronously. gibbs is : \u26a1\ufe0f Highly performant \ud83d\udd00 Asynchronous \ud83d\udc25 Easy-to-use","title":"Introduction"},{"location":"#installation","text":"","title":"Installation"},{"location":"#latest-version","text":"You can install the latest stable version of the package directly from PyPi with : pip install gibbs","title":"Latest version"},{"location":"#bleeding-edge-version","text":"To install the bleeding-edge version ( main , not released), you can do : pip install git+https://github.com/astariul/gibbs.git","title":"Bleeding-edge version"},{"location":"#local-version","text":"For development purposes, you can clone the repository locally and install it manually : git clone https://github.com/astariul/gibbs.git cd gibbs pip install -e .","title":"Local version"},{"location":"#extra-dependencies","text":"You can also install extras dependencies, for example : pip install gibbs [ docs ] Will install necessary dependencies for building the docs. List of extra dependencies : test : Dependencies for running unit-tests. hook : Dependencies for running pre-commit hooks. lint : Dependencies for running linters and formatters. docs : Dependencies for building the documentation. ex : Dependencies for running the examples. dev : test + hook + lint + docs . all : All extra dependencies.","title":"Extra dependencies"},{"location":"#contribute","text":"To contribute, install the package locally (see Installation ), create your own branch, add your code (and tests, and documentation), and open a PR !","title":"Contribute"},{"location":"#pre-commit-hooks","text":"Pre-commit hooks are set to check the code added whenever you commit something. When you try to commit your code, hooks are automatically run, and if you code does not meet the quality required by linters, it will not be committed. You then have to fix your code and try to commit again ! Important If you never ran the hooks before, install it with : pre-commit install Info You can manually run the pre-commit hooks with : pre-commit run --all-files","title":"Pre-commit hooks"},{"location":"#unit-tests","text":"When you contribute, you need to make sure all the unit-tests pass. You should also add tests if necessary ! You can run the tests with : pytest Info Pre-commit hooks will not run the tests, but it will automatically update the coverage badge !","title":"Unit-tests"},{"location":"#documentation","text":"When you contribute, make sure to keep the documentation up-to-date. You can visualize the documentation locally by running : mkdocs serve","title":"Documentation"},{"location":"advanced/","text":"Advanced Changing the port used by gibbs By default, gibbs uses the port 5019 for the sockets the communicate. You can change this port by using the argument gibbs_port in the Worker constructor, and the argument port in the Hub constructor : hub = Hub ( port = 6222 ) w = Worker ( MyModel , gibbs_port = 6222 ) Passing arguments to model's constructor If your model requires arguments for the constructor (like in the example given in the Usage section ) : class MyAwesomeModel : def __init__ ( self , wait_time = 0.25 ): super () . __init__ () self . w = wait_time You can just pass the arguments to the Worker (positional arguments or keyword arguments, both work) : # With positional argument w1 = Worker ( MyAwesomeModel , 0.3 ) # With keyword argument w2 = Worker ( MyAwesomeModel , wait_time = 0.3 ) Only a small list of keywords arguments are reserved for gibbs . Here is the exhaustive list : gibbs_host gibbs_port gibbs_heartbeat_interval gibbs_reset_after_n_miss Starting workers in another machine By default, gibbs workers try to connect to a Hub on the local machine ( localhost ). But you can change this behavior, to have workers running in another machine ! To do this, simply start your worker with the appropriate host using the gibbs_host argument : w = Worker ( MyModel , gibbs_host = \"192.178.0.3\" ) Retrials & timeouts By default, the request() method of the Hub will indefinitely waits for a response from the worker. If instead you want to have a timeout, you can specify the timeout (in seconds ) with the gibbs_timeout argument : h = Hub () await h . request ( x , gibbs_timeout = 0.4 ) If the Hub doesn't receive any response from the worker within the specified timeout, a asyncio.TimeoutError exception is raised. You can also specify a number of retries with the argument gibbs_retries : h = Hub () await h . request ( x , gibbs_timeout = 2 , gibbs_retries = 3 ) With this code, the Hub will try to send a request. If the Hub didn't receive an answer within 2 seconds, it will retry the request again, up to 3 times. Note You need to specify gibbs_timeout when using gibbs_retries , because there is no timeout by default ! You can also specify gibbs_retries=-1 for infinite retries ! Logging Inside gibbs , the library loguru is used for logging. By default, the logger is disabled ( so logging functions become no-op ). If you want to see what's going on inside gibbs (for contributing or debugging for example), you can activate it : from loguru import logger logger . enable ( \"gibbs\" ) # From here, logs will be displayed You can also change the level of logs you want, the format, etc... from loguru import logger import sys logger . enable ( \"gibbs\" ) logger . remove () logger . add ( sys . stderr , format = \" {time} {level} {message} \" , level = \"INFO\" ) Tip For more details, check out the loguru library ! Changing the heartbeat interval By default, the heartbeat interval is set to one second . You can change this value by using the argument gibbs_heartbeat_interval in the Worker constructor, and the argument heartbeat_interval in the Hub constructor : hub = Hub ( heartbeat_interval = 10 ) w = Worker ( MyModel , gibbs_heartbeat_interval = 10 ) Warning Make sure that both the Hub and the workers have the same value for the heartbeat interval, otherwise they might have synchronisation issues !","title":"Advanced"},{"location":"advanced/#advanced","text":"","title":"Advanced"},{"location":"advanced/#changing-the-port-used-by-gibbs","text":"By default, gibbs uses the port 5019 for the sockets the communicate. You can change this port by using the argument gibbs_port in the Worker constructor, and the argument port in the Hub constructor : hub = Hub ( port = 6222 ) w = Worker ( MyModel , gibbs_port = 6222 )","title":"Changing the port used by gibbs"},{"location":"advanced/#passing-arguments-to-models-constructor","text":"If your model requires arguments for the constructor (like in the example given in the Usage section ) : class MyAwesomeModel : def __init__ ( self , wait_time = 0.25 ): super () . __init__ () self . w = wait_time You can just pass the arguments to the Worker (positional arguments or keyword arguments, both work) : # With positional argument w1 = Worker ( MyAwesomeModel , 0.3 ) # With keyword argument w2 = Worker ( MyAwesomeModel , wait_time = 0.3 ) Only a small list of keywords arguments are reserved for gibbs . Here is the exhaustive list : gibbs_host gibbs_port gibbs_heartbeat_interval gibbs_reset_after_n_miss","title":"Passing arguments to model's constructor"},{"location":"advanced/#starting-workers-in-another-machine","text":"By default, gibbs workers try to connect to a Hub on the local machine ( localhost ). But you can change this behavior, to have workers running in another machine ! To do this, simply start your worker with the appropriate host using the gibbs_host argument : w = Worker ( MyModel , gibbs_host = \"192.178.0.3\" )","title":"Starting workers in another machine"},{"location":"advanced/#retrials-timeouts","text":"By default, the request() method of the Hub will indefinitely waits for a response from the worker. If instead you want to have a timeout, you can specify the timeout (in seconds ) with the gibbs_timeout argument : h = Hub () await h . request ( x , gibbs_timeout = 0.4 ) If the Hub doesn't receive any response from the worker within the specified timeout, a asyncio.TimeoutError exception is raised. You can also specify a number of retries with the argument gibbs_retries : h = Hub () await h . request ( x , gibbs_timeout = 2 , gibbs_retries = 3 ) With this code, the Hub will try to send a request. If the Hub didn't receive an answer within 2 seconds, it will retry the request again, up to 3 times. Note You need to specify gibbs_timeout when using gibbs_retries , because there is no timeout by default ! You can also specify gibbs_retries=-1 for infinite retries !","title":"Retrials &amp; timeouts"},{"location":"advanced/#logging","text":"Inside gibbs , the library loguru is used for logging. By default, the logger is disabled ( so logging functions become no-op ). If you want to see what's going on inside gibbs (for contributing or debugging for example), you can activate it : from loguru import logger logger . enable ( \"gibbs\" ) # From here, logs will be displayed You can also change the level of logs you want, the format, etc... from loguru import logger import sys logger . enable ( \"gibbs\" ) logger . remove () logger . add ( sys . stderr , format = \" {time} {level} {message} \" , level = \"INFO\" ) Tip For more details, check out the loguru library !","title":"Logging"},{"location":"advanced/#changing-the-heartbeat-interval","text":"By default, the heartbeat interval is set to one second . You can change this value by using the argument gibbs_heartbeat_interval in the Worker constructor, and the argument heartbeat_interval in the Hub constructor : hub = Hub ( heartbeat_interval = 10 ) w = Worker ( MyModel , gibbs_heartbeat_interval = 10 ) Warning Make sure that both the Hub and the workers have the same value for the heartbeat interval, otherwise they might have synchronisation issues !","title":"Changing the heartbeat interval"},{"location":"architecture/","text":"Architecture Warning This page contains advanced technical details about gibbs . You can probably skip it if you just want to use the library. Tooling gibbs relies on the zmq library for communication between the hub and the workers. zmq is a low-level networking library, providing us with TCP sockets. This gives us performances, and the ability to have workers on different machines. Pattern gibbs implement a modified version of the Paranoid Pirate Pattern . You can read more about the Paranoid Pirate Pattern in the zmq guide . But basically : It is a reliable pattern (it can handle failures) It relies on the REQUEST - REPLY sockets (and their asynchronous equivalent : ROUTER - DEALER ) It automatically balance requests across workers as they come Here is a schema for the Paranoid Pirate Pattern implemented in gibbs : Tip As you can see the the original Paranoid Pirate Pattern is slightly modified : the clients and the queue are merged into a single component, called \"Hub\". Let's see how these components interact with each other to deal with parallel requests : The Hub simply keeps a list of workers that are ready, and send incoming requests to one of the ready worker. Each worker deal with the request it receives. So with two workers, we can deal with two requests in parallel, as shown in the figure above. When receiving the response from the worker, the Hub marks it as ready again. Heartbeat In the zmq guide , the Paranoid Pirate Pattern implements a One-way heartbeat . In gibbs though, the heartbeat is implemented as a Ping-pong heartbeat . Question Heartbeat is necessary to have robustness , in case of workers or Hub crash. Here is how heartbeat works : The workers always initiate the heartbeat ( ping ), and the hub answer it ( pong ). If the worker keeps sending pings but does not receive pongs, we know the Hub is dead. In this case the worker will try to reconnect his socket. So when the Hub is restarted, the worker will automatically reconnect. If the Hub didn't receive a heartbeat from some time, we know this worker is dead. In this case the worker is removed from the list of workers ready, so no requests are sent to this worker. Warning There is a small time interval where a worker can die and the Hub still thinks it's alive. If a request is sent in this interval, the request will fail. To solve this, you can check the section about automatic retrials . Graceful termination In gibbs , the workers terminate cleanly when interrupted with a CTRL-C or another signal such as SIGTERM . By default, these signals simply kill the process, but in our case this is detrimental : if the worker took a request, we want this request to be fully treated, otherwise it is lost. If a worker took a request and then receive a signal to shut down, it will first process the request and send back the response before shutting down. Tip If you don't want your worker to end gracefully, you can always send a SIGKILL . To have this feature, the workers uses another socket internally, and uses polling to get whatever comes first (request or termination signal) : So if a request comes first, the worker will first deal with the request before dealing with the termination signal (shutting down).","title":"Architecture"},{"location":"architecture/#architecture","text":"Warning This page contains advanced technical details about gibbs . You can probably skip it if you just want to use the library.","title":"Architecture"},{"location":"architecture/#tooling","text":"gibbs relies on the zmq library for communication between the hub and the workers. zmq is a low-level networking library, providing us with TCP sockets. This gives us performances, and the ability to have workers on different machines.","title":"Tooling"},{"location":"architecture/#pattern","text":"gibbs implement a modified version of the Paranoid Pirate Pattern . You can read more about the Paranoid Pirate Pattern in the zmq guide . But basically : It is a reliable pattern (it can handle failures) It relies on the REQUEST - REPLY sockets (and their asynchronous equivalent : ROUTER - DEALER ) It automatically balance requests across workers as they come Here is a schema for the Paranoid Pirate Pattern implemented in gibbs : Tip As you can see the the original Paranoid Pirate Pattern is slightly modified : the clients and the queue are merged into a single component, called \"Hub\". Let's see how these components interact with each other to deal with parallel requests : The Hub simply keeps a list of workers that are ready, and send incoming requests to one of the ready worker. Each worker deal with the request it receives. So with two workers, we can deal with two requests in parallel, as shown in the figure above. When receiving the response from the worker, the Hub marks it as ready again.","title":"Pattern"},{"location":"architecture/#heartbeat","text":"In the zmq guide , the Paranoid Pirate Pattern implements a One-way heartbeat . In gibbs though, the heartbeat is implemented as a Ping-pong heartbeat . Question Heartbeat is necessary to have robustness , in case of workers or Hub crash. Here is how heartbeat works : The workers always initiate the heartbeat ( ping ), and the hub answer it ( pong ). If the worker keeps sending pings but does not receive pongs, we know the Hub is dead. In this case the worker will try to reconnect his socket. So when the Hub is restarted, the worker will automatically reconnect. If the Hub didn't receive a heartbeat from some time, we know this worker is dead. In this case the worker is removed from the list of workers ready, so no requests are sent to this worker. Warning There is a small time interval where a worker can die and the Hub still thinks it's alive. If a request is sent in this interval, the request will fail. To solve this, you can check the section about automatic retrials .","title":"Heartbeat"},{"location":"architecture/#graceful-termination","text":"In gibbs , the workers terminate cleanly when interrupted with a CTRL-C or another signal such as SIGTERM . By default, these signals simply kill the process, but in our case this is detrimental : if the worker took a request, we want this request to be fully treated, otherwise it is lost. If a worker took a request and then receive a signal to shut down, it will first process the request and send back the response before shutting down. Tip If you don't want your worker to end gracefully, you can always send a SIGKILL . To have this feature, the workers uses another socket internally, and uses polling to get whatever comes first (request or termination signal) : So if a request comes first, the worker will first deal with the request before dealing with the termination signal (shutting down).","title":"Graceful termination"},{"location":"code_ref/","text":"Code reference API Internal UserCodeException Bases: Exception Custom Exception for user-defined catched errors. Parameters: Name Type Description Default t str Traceback returned by the worker. required Source code in gibbs/hub.py 18 19 20 21 22 23 24 25 26 class UserCodeException ( Exception ): \"\"\"Custom Exception for user-defined catched errors. Args: t (str): Traceback returned by the worker. \"\"\" def __init__ ( self , t : str ): super () . __init__ ( f \"Exception raised in user-defined code. Traceback : \\n { t } \" ) WorkerManager A helper class that takes care of managing workers. Workers' address can be registered as available, and this class will make sure to return address of workers that are available and alive. A worker is considered as dead if we didn't receive any heartbeat within a given interval. Parameters: Name Type Description Default heartbeat_interval float Interval of time (in seconds) after which we consider a worker to be dead. required Source code in gibbs/hub.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class WorkerManager : \"\"\"A helper class that takes care of managing workers. Workers' address can be registered as available, and this class will make sure to return address of workers that are available and alive. A worker is considered as dead if we didn't receive any heartbeat within a given interval. Args: heartbeat_interval (float): Interval of time (in seconds) after which we consider a worker to be dead. \"\"\" def __init__ ( self , heartbeat_interval : float ): super () . __init__ () self . heartbeat_t = heartbeat_interval self . w_ts = {} self . w_access = asyncio . Condition () async def reckon ( self , address : str ): \"\"\"Register the given address as available. Args: address (str): Address of the worker to register as available. \"\"\" async with self . w_access : self . w_ts [ address ] = time . time () self . w_access . notify () async def get_next_worker ( self ) -> str : \"\"\"Retrieve the next available and alive worker's address. Returns: str: Address of the available and alive worker. \"\"\" async with self . w_access : # Iterate workers until we find one that was alive recently w_alive = False while not w_alive : # If no workers are available, wait... if not self . w_ts : await self . w_access . wait () address , ts = self . w_ts . popitem () w_alive = time . time () - ts < self . heartbeat_t return address reckon ( address ) async Register the given address as available. Parameters: Name Type Description Default address str Address of the worker to register as available. required Source code in gibbs/hub.py 50 51 52 53 54 55 56 57 58 async def reckon ( self , address : str ): \"\"\"Register the given address as available. Args: address (str): Address of the worker to register as available. \"\"\" async with self . w_access : self . w_ts [ address ] = time . time () self . w_access . notify () get_next_worker () async Retrieve the next available and alive worker's address. Returns: Name Type Description str str Address of the available and alive worker. Source code in gibbs/hub.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 async def get_next_worker ( self ) -> str : \"\"\"Retrieve the next available and alive worker's address. Returns: str: Address of the available and alive worker. \"\"\" async with self . w_access : # Iterate workers until we find one that was alive recently w_alive = False while not w_alive : # If no workers are available, wait... if not self . w_ts : await self . w_access . wait () address , ts = self . w_ts . popitem () w_alive = time . time () - ts < self . heartbeat_t return address RequestManager A helper class that takes care of storing responses and waiting for the right response. Parameters: Name Type Description Default resp_buffer_size int Maximum size of the response buffer. required Source code in gibbs/hub.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class RequestManager : \"\"\"A helper class that takes care of storing responses and waiting for the right response. Args: resp_buffer_size (int): Maximum size of the response buffer. \"\"\" def __init__ ( self , resp_buffer_size : int ): super () . __init__ () self . resp_buffer_size = resp_buffer_size self . responses = {} self . req_states = {} def pin ( self , req_id : str ): \"\"\"Pin a request ID. This is a necessary step when sending a request, so that the request can be awaited until a response is received. This method should be called for each `req_id` before calling `wait_for`. Args: req_id (str): Request unique identifier. \"\"\" self . req_states [ req_id ] = asyncio . Event () # Ensure we don't store too many requests if len ( self . req_states ) > self . resp_buffer_size : # If it's the case, forget the oldest one k = list ( self . req_states . keys ())[ 0 ] logger . warning ( f \"Response buffer overflow (> { self . resp_buffer_size } ). Forgetting oldest request : { k } \" ) self . req_states . pop ( k ) self . responses . pop ( k , None ) async def wait_for ( self , req_id : str ) -> Tuple [ int , Any ]: \"\"\"Async method that waits until we received the response corresponding to the given request ID. The method `pin` should be called before waiting with this method. Args: req_id (str): Request unique identifier. Raises: KeyError: Exception raised if the request wasn't registered previously. Returns: Tuple[int, Any]: Code and content of the received response. \"\"\" if req_id not in self . req_states : raise KeyError ( f \"Request # { req_id } was not pinned, or was removed because of buffer overflow\" ) # Wait for the receiving loop to receive the response await self . req_states [ req_id ] . wait () # Once we get it, access the result r = self . responses . pop ( req_id ) # Don't forget to remove the event self . req_states . pop ( req_id ) return r . code , r . content def store ( self , req_id : str , code : int , response : Any ): \"\"\"Store a response, to be consumed later. Args: req_id (str): Request unique identifier. code (int): Code of the response. response (Any): Content of the response. \"\"\" # Store the response if the req_id is recognized if req_id in self . req_states : self . responses [ req_id ] = Response ( code , response ) # Notify that we received the response self . req_states [ req_id ] . set () else : logger . warning ( f \"Request # { req_id } was previously removed from response buffer. \" f \"Ignoring the response from this request...\" ) pin ( req_id ) Pin a request ID. This is a necessary step when sending a request, so that the request can be awaited until a response is received. This method should be called for each req_id before calling wait_for . Parameters: Name Type Description Default req_id str Request unique identifier. required Source code in gibbs/hub.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def pin ( self , req_id : str ): \"\"\"Pin a request ID. This is a necessary step when sending a request, so that the request can be awaited until a response is received. This method should be called for each `req_id` before calling `wait_for`. Args: req_id (str): Request unique identifier. \"\"\" self . req_states [ req_id ] = asyncio . Event () # Ensure we don't store too many requests if len ( self . req_states ) > self . resp_buffer_size : # If it's the case, forget the oldest one k = list ( self . req_states . keys ())[ 0 ] logger . warning ( f \"Response buffer overflow (> { self . resp_buffer_size } ). Forgetting oldest request : { k } \" ) self . req_states . pop ( k ) self . responses . pop ( k , None ) wait_for ( req_id ) async Async method that waits until we received the response corresponding to the given request ID. The method pin should be called before waiting with this method. Parameters: Name Type Description Default req_id str Request unique identifier. required Raises: Type Description KeyError Exception raised if the request wasn't registered previously. Returns: Type Description Tuple [ int , Any ] Tuple[int, Any]: Code and content of the received response. Source code in gibbs/hub.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 async def wait_for ( self , req_id : str ) -> Tuple [ int , Any ]: \"\"\"Async method that waits until we received the response corresponding to the given request ID. The method `pin` should be called before waiting with this method. Args: req_id (str): Request unique identifier. Raises: KeyError: Exception raised if the request wasn't registered previously. Returns: Tuple[int, Any]: Code and content of the received response. \"\"\" if req_id not in self . req_states : raise KeyError ( f \"Request # { req_id } was not pinned, or was removed because of buffer overflow\" ) # Wait for the receiving loop to receive the response await self . req_states [ req_id ] . wait () # Once we get it, access the result r = self . responses . pop ( req_id ) # Don't forget to remove the event self . req_states . pop ( req_id ) return r . code , r . content store ( req_id , code , response ) Store a response, to be consumed later. Parameters: Name Type Description Default req_id str Request unique identifier. required code int Code of the response. required response Any Content of the response. required Source code in gibbs/hub.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def store ( self , req_id : str , code : int , response : Any ): \"\"\"Store a response, to be consumed later. Args: req_id (str): Request unique identifier. code (int): Code of the response. response (Any): Content of the response. \"\"\" # Store the response if the req_id is recognized if req_id in self . req_states : self . responses [ req_id ] = Response ( code , response ) # Notify that we received the response self . req_states [ req_id ] . set () else : logger . warning ( f \"Request # { req_id } was previously removed from response buffer. \" f \"Ignoring the response from this request...\" ) Constants RESPONSE_BUFFER_SIZE : int = 4096 module-attribute DEFAULT_PORT : int = 5019 module-attribute DEFAULT_HEARTBEAT_INTERVAL : float = 1 module-attribute DEFAULT_RESET_AFTER_N_MISS : int = 2 module-attribute MS : int = 1000 module-attribute CODE_SUCCESS : int = 0 module-attribute CODE_FAILURE : int = 1 module-attribute PING : bytes = b '' module-attribute PONG : bytes = b '' module-attribute","title":"Code reference"},{"location":"code_ref/#code-reference","text":"","title":"Code reference"},{"location":"code_ref/#api","text":"","title":"API"},{"location":"code_ref/#internal","text":"","title":"Internal"},{"location":"code_ref/#gibbs.hub.UserCodeException","text":"Bases: Exception Custom Exception for user-defined catched errors. Parameters: Name Type Description Default t str Traceback returned by the worker. required Source code in gibbs/hub.py 18 19 20 21 22 23 24 25 26 class UserCodeException ( Exception ): \"\"\"Custom Exception for user-defined catched errors. Args: t (str): Traceback returned by the worker. \"\"\" def __init__ ( self , t : str ): super () . __init__ ( f \"Exception raised in user-defined code. Traceback : \\n { t } \" )","title":"UserCodeException"},{"location":"code_ref/#gibbs.hub.WorkerManager","text":"A helper class that takes care of managing workers. Workers' address can be registered as available, and this class will make sure to return address of workers that are available and alive. A worker is considered as dead if we didn't receive any heartbeat within a given interval. Parameters: Name Type Description Default heartbeat_interval float Interval of time (in seconds) after which we consider a worker to be dead. required Source code in gibbs/hub.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class WorkerManager : \"\"\"A helper class that takes care of managing workers. Workers' address can be registered as available, and this class will make sure to return address of workers that are available and alive. A worker is considered as dead if we didn't receive any heartbeat within a given interval. Args: heartbeat_interval (float): Interval of time (in seconds) after which we consider a worker to be dead. \"\"\" def __init__ ( self , heartbeat_interval : float ): super () . __init__ () self . heartbeat_t = heartbeat_interval self . w_ts = {} self . w_access = asyncio . Condition () async def reckon ( self , address : str ): \"\"\"Register the given address as available. Args: address (str): Address of the worker to register as available. \"\"\" async with self . w_access : self . w_ts [ address ] = time . time () self . w_access . notify () async def get_next_worker ( self ) -> str : \"\"\"Retrieve the next available and alive worker's address. Returns: str: Address of the available and alive worker. \"\"\" async with self . w_access : # Iterate workers until we find one that was alive recently w_alive = False while not w_alive : # If no workers are available, wait... if not self . w_ts : await self . w_access . wait () address , ts = self . w_ts . popitem () w_alive = time . time () - ts < self . heartbeat_t return address","title":"WorkerManager"},{"location":"code_ref/#gibbs.hub.WorkerManager.reckon","text":"Register the given address as available. Parameters: Name Type Description Default address str Address of the worker to register as available. required Source code in gibbs/hub.py 50 51 52 53 54 55 56 57 58 async def reckon ( self , address : str ): \"\"\"Register the given address as available. Args: address (str): Address of the worker to register as available. \"\"\" async with self . w_access : self . w_ts [ address ] = time . time () self . w_access . notify ()","title":"reckon()"},{"location":"code_ref/#gibbs.hub.WorkerManager.get_next_worker","text":"Retrieve the next available and alive worker's address. Returns: Name Type Description str str Address of the available and alive worker. Source code in gibbs/hub.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 async def get_next_worker ( self ) -> str : \"\"\"Retrieve the next available and alive worker's address. Returns: str: Address of the available and alive worker. \"\"\" async with self . w_access : # Iterate workers until we find one that was alive recently w_alive = False while not w_alive : # If no workers are available, wait... if not self . w_ts : await self . w_access . wait () address , ts = self . w_ts . popitem () w_alive = time . time () - ts < self . heartbeat_t return address","title":"get_next_worker()"},{"location":"code_ref/#gibbs.hub.RequestManager","text":"A helper class that takes care of storing responses and waiting for the right response. Parameters: Name Type Description Default resp_buffer_size int Maximum size of the response buffer. required Source code in gibbs/hub.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class RequestManager : \"\"\"A helper class that takes care of storing responses and waiting for the right response. Args: resp_buffer_size (int): Maximum size of the response buffer. \"\"\" def __init__ ( self , resp_buffer_size : int ): super () . __init__ () self . resp_buffer_size = resp_buffer_size self . responses = {} self . req_states = {} def pin ( self , req_id : str ): \"\"\"Pin a request ID. This is a necessary step when sending a request, so that the request can be awaited until a response is received. This method should be called for each `req_id` before calling `wait_for`. Args: req_id (str): Request unique identifier. \"\"\" self . req_states [ req_id ] = asyncio . Event () # Ensure we don't store too many requests if len ( self . req_states ) > self . resp_buffer_size : # If it's the case, forget the oldest one k = list ( self . req_states . keys ())[ 0 ] logger . warning ( f \"Response buffer overflow (> { self . resp_buffer_size } ). Forgetting oldest request : { k } \" ) self . req_states . pop ( k ) self . responses . pop ( k , None ) async def wait_for ( self , req_id : str ) -> Tuple [ int , Any ]: \"\"\"Async method that waits until we received the response corresponding to the given request ID. The method `pin` should be called before waiting with this method. Args: req_id (str): Request unique identifier. Raises: KeyError: Exception raised if the request wasn't registered previously. Returns: Tuple[int, Any]: Code and content of the received response. \"\"\" if req_id not in self . req_states : raise KeyError ( f \"Request # { req_id } was not pinned, or was removed because of buffer overflow\" ) # Wait for the receiving loop to receive the response await self . req_states [ req_id ] . wait () # Once we get it, access the result r = self . responses . pop ( req_id ) # Don't forget to remove the event self . req_states . pop ( req_id ) return r . code , r . content def store ( self , req_id : str , code : int , response : Any ): \"\"\"Store a response, to be consumed later. Args: req_id (str): Request unique identifier. code (int): Code of the response. response (Any): Content of the response. \"\"\" # Store the response if the req_id is recognized if req_id in self . req_states : self . responses [ req_id ] = Response ( code , response ) # Notify that we received the response self . req_states [ req_id ] . set () else : logger . warning ( f \"Request # { req_id } was previously removed from response buffer. \" f \"Ignoring the response from this request...\" )","title":"RequestManager"},{"location":"code_ref/#gibbs.hub.RequestManager.pin","text":"Pin a request ID. This is a necessary step when sending a request, so that the request can be awaited until a response is received. This method should be called for each req_id before calling wait_for . Parameters: Name Type Description Default req_id str Request unique identifier. required Source code in gibbs/hub.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def pin ( self , req_id : str ): \"\"\"Pin a request ID. This is a necessary step when sending a request, so that the request can be awaited until a response is received. This method should be called for each `req_id` before calling `wait_for`. Args: req_id (str): Request unique identifier. \"\"\" self . req_states [ req_id ] = asyncio . Event () # Ensure we don't store too many requests if len ( self . req_states ) > self . resp_buffer_size : # If it's the case, forget the oldest one k = list ( self . req_states . keys ())[ 0 ] logger . warning ( f \"Response buffer overflow (> { self . resp_buffer_size } ). Forgetting oldest request : { k } \" ) self . req_states . pop ( k ) self . responses . pop ( k , None )","title":"pin()"},{"location":"code_ref/#gibbs.hub.RequestManager.wait_for","text":"Async method that waits until we received the response corresponding to the given request ID. The method pin should be called before waiting with this method. Parameters: Name Type Description Default req_id str Request unique identifier. required Raises: Type Description KeyError Exception raised if the request wasn't registered previously. Returns: Type Description Tuple [ int , Any ] Tuple[int, Any]: Code and content of the received response. Source code in gibbs/hub.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 async def wait_for ( self , req_id : str ) -> Tuple [ int , Any ]: \"\"\"Async method that waits until we received the response corresponding to the given request ID. The method `pin` should be called before waiting with this method. Args: req_id (str): Request unique identifier. Raises: KeyError: Exception raised if the request wasn't registered previously. Returns: Tuple[int, Any]: Code and content of the received response. \"\"\" if req_id not in self . req_states : raise KeyError ( f \"Request # { req_id } was not pinned, or was removed because of buffer overflow\" ) # Wait for the receiving loop to receive the response await self . req_states [ req_id ] . wait () # Once we get it, access the result r = self . responses . pop ( req_id ) # Don't forget to remove the event self . req_states . pop ( req_id ) return r . code , r . content","title":"wait_for()"},{"location":"code_ref/#gibbs.hub.RequestManager.store","text":"Store a response, to be consumed later. Parameters: Name Type Description Default req_id str Request unique identifier. required code int Code of the response. required response Any Content of the response. required Source code in gibbs/hub.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def store ( self , req_id : str , code : int , response : Any ): \"\"\"Store a response, to be consumed later. Args: req_id (str): Request unique identifier. code (int): Code of the response. response (Any): Content of the response. \"\"\" # Store the response if the req_id is recognized if req_id in self . req_states : self . responses [ req_id ] = Response ( code , response ) # Notify that we received the response self . req_states [ req_id ] . set () else : logger . warning ( f \"Request # { req_id } was previously removed from response buffer. \" f \"Ignoring the response from this request...\" )","title":"store()"},{"location":"code_ref/#constants","text":"","title":"Constants"},{"location":"code_ref/#gibbs.hub.RESPONSE_BUFFER_SIZE","text":"","title":"RESPONSE_BUFFER_SIZE"},{"location":"code_ref/#gibbs.worker.DEFAULT_PORT","text":"","title":"DEFAULT_PORT"},{"location":"code_ref/#gibbs.worker.DEFAULT_HEARTBEAT_INTERVAL","text":"","title":"DEFAULT_HEARTBEAT_INTERVAL"},{"location":"code_ref/#gibbs.worker.DEFAULT_RESET_AFTER_N_MISS","text":"","title":"DEFAULT_RESET_AFTER_N_MISS"},{"location":"code_ref/#gibbs.worker.MS","text":"","title":"MS"},{"location":"code_ref/#gibbs.worker.CODE_SUCCESS","text":"","title":"CODE_SUCCESS"},{"location":"code_ref/#gibbs.worker.CODE_FAILURE","text":"","title":"CODE_FAILURE"},{"location":"code_ref/#gibbs.worker.PING","text":"","title":"PING"},{"location":"code_ref/#gibbs.worker.PONG","text":"","title":"PONG"},{"location":"examples/","text":"Examples All examples are located in the examples/ folder. All examples can be run directly without any arguments. You can experiment new settings by changing the constants inside the scripts. Important Some examples require additional dependencies. You can install these dependencies with pip install gibbs[ex] (see Installation ) vanilla_fastapi.py python examples/vanilla_fastapi.py This example simply creates a FastAPI application with a dummy model. The dummy model simulate some computations time. The script will measure the time needed to send and receive 10 requests. gibbs_fastapi.py python examples/gibbs_fastapi.py This example is the same as vanilla_fastapi.py , but it uses gibbs to scale up the dummy model (with 2 workers). The script will also measure the time needed to send and receive 10 requests, so you can compare the results with the vanilla approach. transformer.py python examples/transformer.py A more \"real-life\" application, where we use a BART model (from transformers library) along with gibbs for scaling up text-summarization.","title":"Examples"},{"location":"examples/#examples","text":"All examples are located in the examples/ folder. All examples can be run directly without any arguments. You can experiment new settings by changing the constants inside the scripts. Important Some examples require additional dependencies. You can install these dependencies with pip install gibbs[ex] (see Installation )","title":"Examples"},{"location":"examples/#vanilla_fastapipy","text":"python examples/vanilla_fastapi.py This example simply creates a FastAPI application with a dummy model. The dummy model simulate some computations time. The script will measure the time needed to send and receive 10 requests.","title":"vanilla_fastapi.py"},{"location":"examples/#gibbs_fastapipy","text":"python examples/gibbs_fastapi.py This example is the same as vanilla_fastapi.py , but it uses gibbs to scale up the dummy model (with 2 workers). The script will also measure the time needed to send and receive 10 requests, so you can compare the results with the vanilla approach.","title":"gibbs_fastapi.py"},{"location":"examples/#transformerpy","text":"python examples/transformer.py A more \"real-life\" application, where we use a BART model (from transformers library) along with gibbs for scaling up text-summarization.","title":"transformer.py"},{"location":"usage/","text":"Usage Let's walk-through an example of how to scale a FastAPI application together ! Note Here we use FastAPI to show how easy it is to integrate gibbs in an asynchronous framework, but gibbs can be used like any asynchronous python code ! Initial application Let's take a simple example to see how we can scale with gibbs . Say we have developed a great ML model. For the simplicity of this example, here is the code of a dummy model : import time class MyAwesomeModel : def __init__ ( self , wait_time = 0.25 ): super () . __init__ () self . w = wait_time def __call__ ( self , x ): time . sleep ( self . w ) return x ** 2 This model simply return the squared input, after simulating a certain processing time. Now, having a model is great, but we want to make it available to our users. To do that, we create an API using FastAPI, serving that model. Here is the code : import time import uvicorn from fastapi import FastAPI class MyAwesomeModel : def __init__ ( self , wait_time = 0.25 ): super () . __init__ () self . w = wait_time def __call__ ( self , x ): time . sleep ( self . w ) return x ** 2 # Instanciate FastAPI app and instanciate our model app = FastAPI () model = MyAwesomeModel () # Define a route that will call our model and return the result @app . get ( \"/request\" ) async def simple_request ( x : int ): return { \"result\" : model ( x )} if __name__ == \"__main__\" : # Run the app uvicorn . run ( app , host = \"0.0.0.0\" , port = 8000 ) You can run this python script and access http://localhost:8000/docs to try the route by yourself. Great ! We are serving our awesome model ! The scaling issue This code is great, but it does not scale. Because our model takes 250ms to deal with every request, you can imagine what happen when 10 clients send one request at the same time... One of the client will have to wait 2.5s before receiving a response ! You can try this out by starting our simple app, and in another terminal, run the following script : import multiprocessing as mp import time import requests def req_process ( i ): r = requests . get ( f \"http://localhost:8000/request?x= { i } \" ) assert r . status_code == 200 return r . json () def time_parallel_requests ( n ): with mp . Pool ( n ) as p : t0 = time . time () p . map ( req_process , range ( n )) t1 = time . time () return t1 - t0 if __name__ == \"__main__\" : t = time_parallel_requests ( 10 ) print ( f \"It tooks { t : .3f } s to process 10 requests\" ) This script simply run 10 requests in parallel and print the time necessary to complete all of them. And as expected : It tooks 2.532s to process 10 requests How gibbs works What we want is simply to have pool of several models, and when one model is busy dealing with a request, instead of waiting for it to finish, we want to call another (idle) model. So we can deal with several requests in parallel, and therefore serve several clients with a low latency ! To achieve this, gibbs introduces 2 classes : Hub Worker The Worker class is just a process, dealing with requests sequentially by calling the awesome model you created. The Hub is the class that orchestrate the requests, sending each request to the right worker (currently idle). Hint You can see a more detailed description of how this work in Architecture Use gibbs to scale up Let's see how to modify our simple app to scale up. We simply have to create a Hub and use it to send requests, and start a few workers with our awesome model ! import time import uvicorn from fastapi import FastAPI from gibbs import Hub , Worker class MyAwesomeModel : def __init__ ( self , wait_time = 0.25 ): super () . __init__ () self . w = wait_time def __call__ ( self , x ): time . sleep ( self . w ) return x ** 2 # Instanciate FastAPI app and instanciate the Hub app = FastAPI () hub = Hub () # Define a route that will call our model and return the result @app . get ( \"/request\" ) async def simple_request ( x : int ): return { \"result\" : await hub . request ( x )} if __name__ == \"__main__\" : # Start the workers (in another process) workers = [ Worker ( MyAwesomeModel ) for _ in range ( 4 )] for w in workers : w . start () # Run the app uvicorn . run ( app , host = \"0.0.0.0\" , port = 8000 ) Quite simple, right ? Now, if we use the same script as before to run 10 requests in parallel in another terminal : It tooks 0.855s to process 10 requests The time needed to deal with 10 requests is greatly reduced, by sharing the work between the 4 workers !","title":"Usage"},{"location":"usage/#usage","text":"Let's walk-through an example of how to scale a FastAPI application together ! Note Here we use FastAPI to show how easy it is to integrate gibbs in an asynchronous framework, but gibbs can be used like any asynchronous python code !","title":"Usage"},{"location":"usage/#initial-application","text":"Let's take a simple example to see how we can scale with gibbs . Say we have developed a great ML model. For the simplicity of this example, here is the code of a dummy model : import time class MyAwesomeModel : def __init__ ( self , wait_time = 0.25 ): super () . __init__ () self . w = wait_time def __call__ ( self , x ): time . sleep ( self . w ) return x ** 2 This model simply return the squared input, after simulating a certain processing time. Now, having a model is great, but we want to make it available to our users. To do that, we create an API using FastAPI, serving that model. Here is the code : import time import uvicorn from fastapi import FastAPI class MyAwesomeModel : def __init__ ( self , wait_time = 0.25 ): super () . __init__ () self . w = wait_time def __call__ ( self , x ): time . sleep ( self . w ) return x ** 2 # Instanciate FastAPI app and instanciate our model app = FastAPI () model = MyAwesomeModel () # Define a route that will call our model and return the result @app . get ( \"/request\" ) async def simple_request ( x : int ): return { \"result\" : model ( x )} if __name__ == \"__main__\" : # Run the app uvicorn . run ( app , host = \"0.0.0.0\" , port = 8000 ) You can run this python script and access http://localhost:8000/docs to try the route by yourself. Great ! We are serving our awesome model !","title":"Initial application"},{"location":"usage/#the-scaling-issue","text":"This code is great, but it does not scale. Because our model takes 250ms to deal with every request, you can imagine what happen when 10 clients send one request at the same time... One of the client will have to wait 2.5s before receiving a response ! You can try this out by starting our simple app, and in another terminal, run the following script : import multiprocessing as mp import time import requests def req_process ( i ): r = requests . get ( f \"http://localhost:8000/request?x= { i } \" ) assert r . status_code == 200 return r . json () def time_parallel_requests ( n ): with mp . Pool ( n ) as p : t0 = time . time () p . map ( req_process , range ( n )) t1 = time . time () return t1 - t0 if __name__ == \"__main__\" : t = time_parallel_requests ( 10 ) print ( f \"It tooks { t : .3f } s to process 10 requests\" ) This script simply run 10 requests in parallel and print the time necessary to complete all of them. And as expected : It tooks 2.532s to process 10 requests","title":"The scaling issue"},{"location":"usage/#how-gibbs-works","text":"What we want is simply to have pool of several models, and when one model is busy dealing with a request, instead of waiting for it to finish, we want to call another (idle) model. So we can deal with several requests in parallel, and therefore serve several clients with a low latency ! To achieve this, gibbs introduces 2 classes : Hub Worker The Worker class is just a process, dealing with requests sequentially by calling the awesome model you created. The Hub is the class that orchestrate the requests, sending each request to the right worker (currently idle). Hint You can see a more detailed description of how this work in Architecture","title":"How gibbs works"},{"location":"usage/#use-gibbs-to-scale-up","text":"Let's see how to modify our simple app to scale up. We simply have to create a Hub and use it to send requests, and start a few workers with our awesome model ! import time import uvicorn from fastapi import FastAPI from gibbs import Hub , Worker class MyAwesomeModel : def __init__ ( self , wait_time = 0.25 ): super () . __init__ () self . w = wait_time def __call__ ( self , x ): time . sleep ( self . w ) return x ** 2 # Instanciate FastAPI app and instanciate the Hub app = FastAPI () hub = Hub () # Define a route that will call our model and return the result @app . get ( \"/request\" ) async def simple_request ( x : int ): return { \"result\" : await hub . request ( x )} if __name__ == \"__main__\" : # Start the workers (in another process) workers = [ Worker ( MyAwesomeModel ) for _ in range ( 4 )] for w in workers : w . start () # Run the app uvicorn . run ( app , host = \"0.0.0.0\" , port = 8000 ) Quite simple, right ? Now, if we use the same script as before to run 10 requests in parallel in another terminal : It tooks 0.855s to process 10 requests The time needed to deal with 10 requests is greatly reduced, by sharing the work between the 4 workers !","title":"Use gibbs to scale up"}]}